{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cc540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198bb5c",
   "metadata": {},
   "source": [
    "## Step 1: Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676c8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b38552",
   "metadata": {},
   "source": [
    "## Step 2: Removing Stop Words and storing them in a separate array of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8052d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots significatifs: ['many', 'techniques', 'available', 'generate', 'extractive', 'summarization', 'keep', 'simple', ',', 'using', 'unsupervised', 'learning', 'approach', 'find', 'sentences', 'similarity', 'rank', '.', 'Summarization', 'defined', 'task', 'producing', 'concise', 'fluent', 'summary', 'preserving', 'key', 'information', 'overall', 'meaning', '.', 'One', 'benefit', ',', '’', 'need', 'train', 'build', 'model', 'prior', 'start', 'using', 'project', '.', '’', 'good', 'understand', 'Cosine', 'similarity', 'make', 'best', 'use', 'code', 'going', 'see', '.', 'Cosine', 'similarity', 'measure', 'similarity', 'two', 'non-zero', 'vectors', 'inner', 'product', 'space', 'measures', 'cosine', 'angle', '.', 'measures', 'cosine', 'angle', 'vectors', '.', 'angle', '0', 'sentences', 'similar', '.']\n",
      "Mots vides: ['There', 'are', 'to', 'to', 'it', 'I', 'will', 'be', 'an', 'to', 'the', 'and', 'them', 'can', 'be', 'as', 'a', 'of', 'a', 'and', 'while', 'and', 'of', 'this', 'will', 'be', 'you', 'don', 't', 'to', 'and', 'a', 'it', 'for', 'your', 'It', 's', 'to', 'to', 'the', 'of', 'the', 'you', 'are', 'to', 'is', 'a', 'of', 'between', 'of', 'an', 'that', 'the', 'of', 'the', 'between', 'them', 'Its', 'of', 'the', 'between', 'The', 'will', 'be', 'if', 'are']\n"
     ]
    }
   ],
   "source": [
    "# Étape 2: Suppression des mots vides et stockage dans un tableau séparé\n",
    "text = \"\"\"There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Summarization can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning. One benefit of this will be, you don’t need to train and build a model prior start using it for your project. It’s good to understand Cosine similarity to make the best use of the code you are going to see. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. The angle will be 0 if sentences are similar.\"\"\"\n",
    "\n",
    "# Étape 3: Création d'une table de fréquence des mots\n",
    "stopWords = set(stopwords.words(\"english\")) \n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Séparation des mots vides et des mots significatifs\n",
    "words_significatifs = [mot for mot in words if mot.lower() not in stopWords]\n",
    "words_vides = [mot for mot in words if mot.lower() in stopWords]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Mots significatifs:\", words_significatifs)\n",
    "print(\"Mots vides:\", words_vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f285f",
   "metadata": {},
   "source": [
    "## Step 3: Create a frequency table of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fca1c0",
   "metadata": {},
   "source": [
    "### A python dictionary that’ll keep a record of how many times each word appears in the feedback after removing the stop words.we can use the dictionary over every sentence to know which sentences have the most relevant content in the overall text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4b4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqTable = dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426abe7",
   "metadata": {},
   "source": [
    "## Step 4: Assign score to each sentence depending on the words it contains and the frequency table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06365ee3",
   "metadata": {},
   "source": [
    "### We can use the sent_tokenize() method to create the array of sentences. Secondly, we will need a dictionary to keep the score of each sentence, we will later go through the dictionary to generate the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d7d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text) \n",
    "sentenceValue = dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bfef6",
   "metadata": {},
   "source": [
    "##  Step 5: Assign a certain score to compare the sentences within the feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7be85",
   "metadata": {},
   "source": [
    "###  A simple approach to compare our scores would be to find the average score of a sentence. The average itself can be a good threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "416f8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumValues = 0\n",
    "for sentence in sentenceValue: \n",
    "    sumValues += sentenceValue[sentence] \n",
    "#average = int(sumValues / len(sentenceValue)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a9ec4",
   "metadata": {},
   "source": [
    "### Apply the threshold value and store sentences in order into the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92fa7a8",
   "metadata": {},
   "source": [
    "## Code : Complete implementation of Text Summarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a76bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Texte d'entrée - à résumer\n",
    "text = \"\"\"There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Summarization can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning. One benefit of this will be, you don’t need to train and build a model prior start using it for your project. It’s good to understand Cosine similarity to make the best use of the code you are going to see. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. The angle will be 0 if sentences are similar.\"\"\"\n",
    "\n",
    "# Vérification si le texte n'est pas vide\n",
    "if not text.strip():\n",
    "    print(\"Le texte d'entrée est vide.\")\n",
    "else:\n",
    "    # Tokenisation du texte\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Création d'une table de fréquence pour garder la trace de chaque mot\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # Création d'un dictionnaire pour garder la trace de chaque phrase\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "\n",
    "    # Calcul de la somme des valeurs de toutes les phrases\n",
    "    sumValues = sum(sentenceValue.values())\n",
    "\n",
    "    # Vérification pour éviter la division par zéro\n",
    "    if len(sentenceValue) == 0:\n",
    "        print(\"Aucune phrase significative n'a été trouvée.\")\n",
    "    else:\n",
    "        # Valeur moyenne d'une phrase du texte original\n",
    "        average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "        # Stockage des phrases dans le résumé\n",
    "        summary = ''\n",
    "        for sentence in sentences:\n",
    "            if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "                summary += \" \" + sentence\n",
    "        print(summary if summary else \"Aucune phrase n'a été sélectionnée pour le résumé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032bf8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
